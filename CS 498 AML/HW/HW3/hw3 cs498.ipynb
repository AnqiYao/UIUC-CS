{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy.linalg as linalg\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_data1 = pd.read_csv(\"dataI.csv\")\n",
    "origin_data2 = pd.read_csv(\"dataII.csv\")\n",
    "origin_data3 = pd.read_csv(\"dataIII.csv\")\n",
    "origin_data4 = pd.read_csv(\"dataIV.csv\")\n",
    "origin_data5 = pd.read_csv(\"dataV.csv\")\n",
    "origin_noisyless= pd.read_csv(\"iris.csv\")\n",
    "\n",
    "data1_noisy= pd.read_csv(\"dataI.csv\")\n",
    "data2_noisy= pd.read_csv(\"dataII.csv\")\n",
    "data3_noisy= pd.read_csv(\"dataIII.csv\")\n",
    "data4_noisy= pd.read_csv(\"dataIV.csv\")\n",
    "data5_noisy= pd.read_csv(\"dataV.csv\")\n",
    "data_noisyless= pd.read_csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not need since we have library\n",
    "#mean and covariance matrix for the noisyless data\n",
    "mean_datanoisy = np.mean(data_noisyless, axis=0)\n",
    "data_noisyless[\"Sepal.Length\"] = data_noisyless[\"Sepal.Length\"] - mean_datanoisy[0]\n",
    "data_noisyless[\"Sepal.Width\"] = data_noisyless[\"Sepal.Width\"] - mean_datanoisy[1]\n",
    "data_noisyless[\"Petal.Length\"] = data_noisyless[\"Petal.Length\"] - mean_datanoisy[2]\n",
    "data_noisyless[\"Petal.Width\"] = data_noisyless[\"Petal.Width\"] - mean_datanoisy[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not need since we have library\n",
    "#function for  covariance matrix \n",
    "def return_covariance_mat(data, number):\n",
    "    covariance_mat_noisyless = np.zeros((number,number))\n",
    "    for i in range(1,len(data.iloc[0])+1):\n",
    "        for j in range(1,len(data.iloc[0])+1):\n",
    "            sum = 0\n",
    "            for k in range(1,len(data)+1):\n",
    "                sum = data.iloc[k-1][i-1] * data.iloc[k-1][j-1]\n",
    "            covariance_mat_noisyless[i-1][j-1] = sum\n",
    "    return covariance_mat_noisyless "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not need since we have library\n",
    "#covariance matrix for the noisyless data\n",
    "covariance_mat_noisyless = return_covariance_mat(data_noisyless, len(data_noisyless.iloc[0]))\n",
    "#print(covariance_mat_noisyless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not need since we have library\n",
    "#mean and covariance matrix for the noisy data \n",
    "mean_data1 = np.mean(data1_noisy, axis=0)\n",
    "mean_data2 = np.mean(data2_noisy, axis=0)\n",
    "mean_data3 = np.mean(data3_noisy, axis=0)\n",
    "mean_data4 = np.mean(data4_noisy, axis=0)\n",
    "mean_data5 = np.mean(data5_noisy, axis=0)\n",
    "\n",
    "data1_noisy[\"X1\"] = data1_noisy[\"X1\"] - mean_data1[0]\n",
    "data1_noisy[\"X2\"] = data1_noisy[\"X2\"] - mean_data1[1]\n",
    "data1_noisy[\"X3\"] = data1_noisy[\"X3\"] - mean_data1[2]\n",
    "data1_noisy[\"X4\"] = data1_noisy[\"X4\"] - mean_data1[3]\n",
    "\n",
    "data2_noisy[\"X1\"] = data2_noisy[\"X1\"] - mean_data2[0]\n",
    "data2_noisy[\"X2\"] = data2_noisy[\"X2\"] - mean_data2[1]\n",
    "data2_noisy[\"X3\"] = data2_noisy[\"X3\"] - mean_data2[2]\n",
    "data2_noisy[\"X4\"] = data2_noisy[\"X4\"] - mean_data2[3]\n",
    "\n",
    "data3_noisy[\"X1\"] = data3_noisy[\"X1\"] - mean_data3[0]\n",
    "data3_noisy[\"X2\"] = data3_noisy[\"X2\"] - mean_data3[1]\n",
    "data3_noisy[\"X3\"] = data3_noisy[\"X3\"] - mean_data3[2]\n",
    "data3_noisy[\"X4\"] = data3_noisy[\"X4\"] - mean_data3[3]\n",
    "\n",
    "data4_noisy[\"X1\"] = data4_noisy[\"X1\"] - mean_data4[0]\n",
    "data4_noisy[\"X2\"] = data4_noisy[\"X2\"] - mean_data4[1]\n",
    "data4_noisy[\"X3\"] = data4_noisy[\"X3\"] - mean_data4[2]\n",
    "data4_noisy[\"X4\"] = data4_noisy[\"X4\"] - mean_data4[3]\n",
    "\n",
    "data5_noisy[\"X1\"] = data5_noisy[\"X1\"] - mean_data5[0]\n",
    "data5_noisy[\"X2\"] = data5_noisy[\"X2\"] - mean_data5[1]\n",
    "data5_noisy[\"X3\"] = data5_noisy[\"X3\"] - mean_data5[2]\n",
    "data5_noisy[\"X4\"] = data5_noisy[\"X4\"] - mean_data5[3]\n",
    "\n",
    "\n",
    "covariance_data1_noisy = return_covariance_mat(data1_noisy, len(data1_noisy.iloc[0]))\n",
    "covariance_data2_noisy = return_covariance_mat(data2_noisy, len(data2_noisy.iloc[0]))\n",
    "covariance_data3_noisy = return_covariance_mat(data3_noisy, len(data3_noisy.iloc[0]))\n",
    "covariance_data4_noisy = return_covariance_mat(data4_noisy, len(data4_noisy.iloc[0]))\n",
    "covariance_data5_noisy = return_covariance_mat(data5_noisy, len(data5_noisy.iloc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is something weird with this function\n",
    "def mean_sqr(a):\n",
    "    result = a - origin_noisyless\n",
    "    for i in range(150):\n",
    "        for j in range(4):\n",
    "            result.iloc[i][j] = result.iloc[i][j] **2\n",
    "    result = np.matrix(result)\n",
    "    return np.matrix.sum(result)/150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.543119029074549\n",
      "4.54311902907455\n",
      "0.3846135339576175\n",
      "0.1778152826696257\n",
      "0.1444405060313772\n",
      "0.1608383618076381\n"
     ]
    }
   ],
   "source": [
    "#StandardScaler.inverse_transform(pca_data1,pc_data1_1)\n",
    "#doing PCA with data_1 \n",
    "pca_data0 = PCA(n_components=0)\n",
    "pc_data1_0 = pca_data0.fit_transform(origin_data1)  #this is before reconstructing, has only 1 column\n",
    "pca0_data1_recons = np.dot(pc_data1_0, pca_data0.components_) + pca_data0.mean_\n",
    "print(mean_sqr(pca0_data1_recons))\n",
    "print(mean_squared_error(pca0_data1_recons, origin_noisyless)*4)\n",
    "\n",
    "\n",
    "pca_data1 = PCA(n_components=1)\n",
    "pc_data1_1 = pca_data1.fit_transform(origin_data1)  #this is before reconstructing, has only 1 column\n",
    "pca1_data1_recons = np.dot(pc_data1_1, pca_data1.components_) + pca_data1.mean_\n",
    "print(mean_squared_error(pca1_data1_recons, origin_noisyless)*4)\n",
    "\n",
    "\n",
    "pca_data2 = PCA(n_components=2)\n",
    "pc_data1_2 = pca_data2.fit_transform(origin_data1)  \n",
    "pca2_data1_recons = np.dot(pc_data1_2, pca_data2.components_) + pca_data2.mean_\n",
    "print(mean_squared_error(pca2_data1_recons, origin_noisyless)*4)\n",
    "\n",
    "\n",
    "pca_data3 = PCA(n_components=3)\n",
    "pc_data1_3 = pca_data3.fit_transform(origin_data1) \n",
    "pca3_data1_recons = np.dot(pc_data1_3, pca_data3.components_) + pca_data3.mean_\n",
    "print(mean_squared_error(pca3_data1_recons, origin_noisyless)*4)\n",
    "\n",
    "\n",
    "pca_data4 = PCA(n_components=4)\n",
    "pc_data1_4 = pca_data4.fit_transform(origin_data1)  \n",
    "pca4_data1_recons = np.dot(pc_data1_4, pca_data4.components_) + pca_data4.mean_\n",
    "print(mean_squared_error(pca4_data1_recons, origin_noisyless)*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.549538992715444\n",
      "0.6486421084108522\n",
      "0.7506211289999839\n",
      "0.941972819285057\n",
      "1.1156578578493086\n"
     ]
    }
   ],
   "source": [
    "#doing PCA with data_2\n",
    "pca2_data0 = PCA(n_components=0)\n",
    "pc_data2_0 = pca2_data0.fit_transform(origin_data2)  #this is before reconstructing, has only 1 column\n",
    "pca0_data2_recons = np.dot(pc_data2_0, pca2_data0.components_) + pca2_data0.mean_\n",
    "print(mean_squared_error(pca0_data2_recons, origin_noisyless)*4)\n",
    "\n",
    "pca2_data1 = PCA(n_components=1)\n",
    "pc_data2_1 = pca2_data1.fit_transform(origin_data2)  #this is before reconstructing, has only 1 column\n",
    "pca1_data2_recons = np.dot(pc_data2_1, pca2_data1.components_) + pca2_data1.mean_\n",
    "print(mean_squared_error(pca1_data2_recons, origin_noisyless)*4)\n",
    "\n",
    "\n",
    "pca2_data2 = PCA(n_components=2)\n",
    "pc_data2_2 = pca2_data2.fit_transform(origin_data2)  \n",
    "pca2_data2_recons = np.dot(pc_data2_2, pca2_data2.components_) + pca2_data2.mean_\n",
    "print(mean_squared_error(pca2_data2_recons, origin_noisyless)*4)\n",
    "df = pd.DataFrame(pca2_data2_recons)\n",
    "df.to_csv(\"data2_n=2.csv\")\n",
    "\n",
    "\n",
    "pca2_data3 = PCA(n_components=3)\n",
    "pc_data2_3 = pca2_data3.fit_transform(origin_data2) \n",
    "pca3_data2_recons = np.dot(pc_data2_3, pca2_data3.components_) + pca2_data3.mean_\n",
    "print(mean_squared_error(pca3_data2_recons, origin_noisyless)*4)\n",
    "\n",
    "\n",
    "pca2_data4 = PCA(n_components=4)\n",
    "pc_data2_4 = pca2_data4.fit_transform(origin_data2)  \n",
    "pca4_data2_recons = np.dot(pc_data2_4, pca2_data4.components_) + pca2_data4.mean_\n",
    "print(mean_squared_error(pca4_data2_recons, origin_noisyless)*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.557472963930545\n",
      "1.323462148041875\n",
      "2.1197480492819545\n",
      "3.027379919975331\n",
      "3.653279732511108\n"
     ]
    }
   ],
   "source": [
    "#doing PCA with data_3\n",
    "pca3_data0 = PCA(n_components=0)\n",
    "pc_data3_0 = pca3_data0.fit_transform(origin_data3)  #this is before reconstructing, has only 1 column\n",
    "pca0_data3_recons = np.dot(pc_data3_0, pca3_data0.components_) + pca3_data0.mean_\n",
    "print(mean_squared_error(pca0_data3_recons, origin_noisyless)*4)\n",
    "\n",
    "pca3_data1 = PCA(n_components=1)\n",
    "pc_data3_1 = pca3_data1.fit_transform(origin_data3)  #this is before reconstructing, has only 1 column\n",
    "pca1_data3_recons = np.dot(pc_data3_1, pca3_data1.components_) + pca3_data1.mean_\n",
    "print(mean_squared_error(pca1_data3_recons, origin_noisyless)*4)\n",
    "\n",
    "\n",
    "pca3_data2 = PCA(n_components=2)\n",
    "pc_data3_2 = pca3_data2.fit_transform(origin_data3)  \n",
    "pca2_data3_recons = np.dot(pc_data3_2, pca3_data2.components_) + pca3_data2.mean_\n",
    "print(mean_squared_error(pca2_data3_recons, origin_noisyless)*4)\n",
    "\n",
    "\n",
    "pca3_data3 = PCA(n_components=3)\n",
    "pc_data3_3 = pca3_data3.fit_transform(origin_data3) \n",
    "pca3_data3_recons = np.dot(pc_data3_3, pca3_data3.components_) + pca3_data3.mean_\n",
    "print(mean_squared_error(pca3_data3_recons, origin_noisyless)*4)\n",
    "\n",
    "\n",
    "pca3_data4 = PCA(n_components=4)\n",
    "pc_data3_4 = pca3_data4.fit_transform(origin_data3)  \n",
    "pca4_data3_recons = np.dot(pc_data3_4, pca3_data4.components_) + pca3_data4.mean_\n",
    "print(mean_squared_error(pca4_data3_recons, origin_noisyless)*4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5661986666666685\n",
      "0.8406141572571983\n",
      "1.2070897968259073\n",
      "1.2711919671860716\n",
      "1.1940000000000008\n"
     ]
    }
   ],
   "source": [
    "#doing PCA with data_4\n",
    "pca4_data0 = PCA(n_components=0)\n",
    "pc_data4_0 = pca4_data0.fit_transform(origin_data4)  #this is before reconstructing, has only 1 column\n",
    "pca0_data4_recons = np.dot(pc_data4_0, pca4_data0.components_) + pca4_data0.mean_\n",
    "print(mean_squared_error(pca0_data4_recons, origin_noisyless)*4)\n",
    "\n",
    "pca4_data1 = PCA(n_components=1)\n",
    "pc_data4_1 = pca4_data1.fit_transform(origin_data4)  #this is before reconstructing, has only 1 column\n",
    "pca1_data4_recons = np.dot(pc_data4_1, pca4_data1.components_) + pca4_data1.mean_\n",
    "print(mean_squared_error(pca1_data4_recons, origin_noisyless)*4)\n",
    "\n",
    "\n",
    "pca4_data2 = PCA(n_components=2)\n",
    "pc_data4_2 = pca4_data2.fit_transform(origin_data4)  \n",
    "pca2_data4_recons = np.dot(pc_data4_2, pca4_data2.components_) + pca4_data2.mean_\n",
    "print(mean_squared_error(pca2_data4_recons, origin_noisyless)*4)\n",
    "\n",
    "\n",
    "pca4_data3 = PCA(n_components=3)\n",
    "pc_data4_3 = pca4_data3.fit_transform(origin_data4) \n",
    "pca3_data4_recons = np.dot(pc_data4_3, pca4_data3.components_) + pca4_data3.mean_\n",
    "print(mean_squared_error(pca3_data4_recons, origin_noisyless)*4)\n",
    "\n",
    "\n",
    "pca4_data4 = PCA(n_components=4)\n",
    "pc_data4_4 = pca4_data4.fit_transform(origin_data4)  \n",
    "pca4_data4_recons = np.dot(pc_data4_4, pca4_data4.components_) + pca4_data4.mean_\n",
    "print(mean_squared_error(pca4_data4_recons, origin_noisyless)*4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9199280000000005\n",
      "2.83567942802643\n",
      "4.651434502717086\n",
      "4.971247271525591\n",
      "5.1392666666666615\n"
     ]
    }
   ],
   "source": [
    "#doing PCA with data_5\n",
    "pca5_data0 = PCA(n_components=0)\n",
    "pc_data5_0 = pca5_data0.fit_transform(origin_data5)  #this is before reconstructing, has only 1 column\n",
    "pca0_data5_recons = np.dot(pc_data5_0, pca5_data0.components_) + pca5_data0.mean_\n",
    "print(mean_squared_error(pca0_data5_recons, origin_noisyless)*4)\n",
    "\n",
    "pca5_data1 = PCA(n_components=1)\n",
    "pc_data5_1 = pca5_data1.fit_transform(origin_data5)  #this is before reconstructing, has only 1 column\n",
    "pca1_data5_recons = np.dot(pc_data5_1, pca5_data1.components_) + pca5_data1.mean_\n",
    "print(mean_squared_error(pca1_data5_recons, origin_noisyless)*4)\n",
    "\n",
    "\n",
    "pca5_data2 = PCA(n_components=2)\n",
    "pc_data5_2 = pca5_data2.fit_transform(origin_data5)  \n",
    "pca2_data5_recons = np.dot(pc_data5_2, pca5_data2.components_) + pca5_data2.mean_\n",
    "print(mean_squared_error(pca2_data5_recons, origin_noisyless)*4)\n",
    "\n",
    "\n",
    "pca5_data3 = PCA(n_components=3)\n",
    "pc_data5_3 = pca5_data3.fit_transform(origin_data5) \n",
    "pca3_data5_recons = np.dot(pc_data5_3, pca5_data3.components_) + pca5_data3.mean_\n",
    "print(mean_squared_error(pca3_data5_recons, origin_noisyless)*4)\n",
    "\n",
    "\n",
    "pca5_data4 = PCA(n_components=4)\n",
    "pc_data5_4 = pca5_data4.fit_transform(origin_data5)  \n",
    "pca4_data5_recons = np.dot(pc_data5_4, pca5_data4.components_) + pca5_data4.mean_\n",
    "print(mean_squared_error(pca4_data5_recons, origin_noisyless)*4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.542470666666669\n",
      "0.38345031150498454\n",
      "0.17556300024433896\n",
      "0.1417836480045701\n",
      "0.1608383618076382\n"
     ]
    }
   ],
   "source": [
    "#noiseless for data1\n",
    "pcan_data1 = PCA(n_components=0)\n",
    "pcan_data1.fit(origin_noisyless)\n",
    "pcant_data1 = pcan_data1.transform(origin_data1)\n",
    "pcan_data1_recons = np.dot(pcant_data1, pcan_data1.components_) + pcan_data1.mean_\n",
    "print(mean_squared_error(pcan_data1_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data1 = PCA(n_components=1)\n",
    "pcan_data1.fit(origin_noisyless)\n",
    "pcant_data1 = pcan_data1.transform(origin_data1)\n",
    "pcan_data1_recons = np.dot(pcant_data1, pcan_data1.components_) + pcan_data1.mean_\n",
    "print(mean_squared_error(pcan_data1_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data1 = PCA(n_components=2)\n",
    "pcan_data1.fit(origin_noisyless)\n",
    "pcant_data1 = pcan_data1.transform(origin_data1)\n",
    "pcan_data1_recons = np.dot(pcant_data1, pcan_data1.components_) + pcan_data1.mean_\n",
    "print(mean_squared_error(pcan_data1_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data1 = PCA(n_components=3)\n",
    "pcan_data1.fit(origin_noisyless)\n",
    "pcant_data1 = pcan_data1.transform(origin_data1)\n",
    "pcan_data1_recons = np.dot(pcant_data1, pcan_data1.components_) + pcan_data1.mean_\n",
    "print(mean_squared_error(pcan_data1_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data1 = PCA(n_components=4)\n",
    "pcan_data1.fit(origin_noisyless)\n",
    "pcant_data1 = pcan_data1.transform(origin_data1)\n",
    "pcan_data1_recons = np.dot(pcant_data1, pcan_data1.components_) + pcan_data1.mean_\n",
    "print(mean_squared_error(pcan_data1_recons, origin_noisyless)*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.542470666666669\n",
      "0.6410931849009851\n",
      "0.7156284875049564\n",
      "0.9083929073982747\n",
      "1.1156578578493088\n"
     ]
    }
   ],
   "source": [
    "#noiseless for data2\n",
    "pcan_data2 = PCA(n_components=0)\n",
    "pcan_data2.fit(origin_noisyless)\n",
    "pcant_data2 = pcan_data2.transform(origin_data2)\n",
    "pcan_data2_recons = np.dot(pcant_data2, pcan_data2.components_) + pcan_data2.mean_\n",
    "print(mean_squared_error(pcan_data2_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data2 = PCA(n_components=1)\n",
    "pcan_data2.fit(origin_noisyless)\n",
    "pcant_data2 = pcan_data2.transform(origin_data2)\n",
    "pcan_data2_recons = np.dot(pcant_data2, pcan_data2.components_) + pcan_data2.mean_\n",
    "print(mean_squared_error(pcan_data2_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data2 = PCA(n_components=2)\n",
    "pcan_data2.fit(origin_noisyless)\n",
    "pcant_data2 = pcan_data2.transform(origin_data2)\n",
    "pcan_data2_recons = np.dot(pcant_data2, pcan_data2.components_) + pcan_data2.mean_\n",
    "print(mean_squared_error(pcan_data2_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data2 = PCA(n_components=3)\n",
    "pcan_data2.fit(origin_noisyless)\n",
    "pcant_data2 = pcan_data2.transform(origin_data2)\n",
    "pcan_data2_recons = np.dot(pcant_data2, pcan_data2.components_) + pcan_data2.mean_\n",
    "print(mean_squared_error(pcan_data2_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data2 = PCA(n_components=4)\n",
    "pcan_data2.fit(origin_noisyless)\n",
    "pcant_data2 = pcan_data2.transform(origin_data2)\n",
    "pcan_data2_recons = np.dot(pcant_data2, pcan_data2.components_) + pcan_data2.mean_\n",
    "print(mean_squared_error(pcan_data2_recons, origin_noisyless)*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.542470666666669\n",
      "1.2903724507598013\n",
      "1.9672403923798707\n",
      "2.6508411351327363\n",
      "3.65327973251111\n"
     ]
    }
   ],
   "source": [
    "#noiseless for data3\n",
    "pcan_data3 = PCA(n_components=0)\n",
    "pcan_data3.fit(origin_noisyless)\n",
    "pcant_data3 = pcan_data3.transform(origin_data3)\n",
    "pcan_data3_recons = np.dot(pcant_data3, pcan_data3.components_) + pcan_data3.mean_\n",
    "print(mean_squared_error(pcan_data3_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data3 = PCA(n_components=1)\n",
    "pcan_data3.fit(origin_noisyless)\n",
    "pcant_data3 = pcan_data3.transform(origin_data3)\n",
    "pcan_data3_recons = np.dot(pcant_data3, pcan_data3.components_) + pcan_data3.mean_\n",
    "print(mean_squared_error(pcan_data3_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data3 = PCA(n_components=2)\n",
    "pcan_data3.fit(origin_noisyless)\n",
    "pcant_data3 = pcan_data3.transform(origin_data3)\n",
    "pcan_data3_recons = np.dot(pcant_data3, pcan_data3.components_) + pcan_data3.mean_\n",
    "print(mean_squared_error(pcan_data3_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data3 = PCA(n_components=3)\n",
    "pcan_data3.fit(origin_noisyless)\n",
    "pcant_data3 = pcan_data3.transform(origin_data3)\n",
    "pcan_data3_recons = np.dot(pcant_data3, pcan_data3.components_) + pcan_data3.mean_\n",
    "print(mean_squared_error(pcan_data3_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data3 = PCA(n_components=4)\n",
    "pcan_data3.fit(origin_noisyless)\n",
    "pcant_data3 = pcan_data3.transform(origin_data3)\n",
    "pcan_data3_recons = np.dot(pcant_data3, pcan_data3.components_) + pcan_data3.mean_\n",
    "print(mean_squared_error(pcan_data3_recons, origin_noisyless)*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.542470666666669\n",
      "0.7999427437338251\n",
      "0.8280825547067427\n",
      "0.9849497682406678\n",
      "1.1940000000000004\n"
     ]
    }
   ],
   "source": [
    "#noiseless for data4\n",
    "pcan_data4 = PCA(n_components=0)\n",
    "pcan_data4.fit(origin_noisyless)\n",
    "pcant_data4 = pcan_data4.transform(origin_data4)\n",
    "pcan_data4_recons = np.dot(pcant_data4, pcan_data4.components_) + pcan_data4.mean_\n",
    "print(mean_squared_error(pcan_data4_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data4 = PCA(n_components=1)\n",
    "pcan_data4.fit(origin_noisyless)\n",
    "pcant_data4 = pcan_data4.transform(origin_data4)\n",
    "pcan_data4_recons = np.dot(pcant_data4, pcan_data4.components_) + pcan_data4.mean_\n",
    "print(mean_squared_error(pcan_data4_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data4 = PCA(n_components=2)\n",
    "pcan_data4.fit(origin_noisyless)\n",
    "pcant_data4 = pcan_data4.transform(origin_data4)\n",
    "pcan_data4_recons = np.dot(pcant_data4, pcan_data4.components_) + pcan_data4.mean_\n",
    "print(mean_squared_error(pcan_data4_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data4 = PCA(n_components=3)\n",
    "pcan_data4.fit(origin_noisyless)\n",
    "pcant_data4 = pcan_data4.transform(origin_data4)\n",
    "pcan_data4_recons = np.dot(pcant_data4, pcan_data4.components_) + pcan_data4.mean_\n",
    "print(mean_squared_error(pcan_data4_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data4 = PCA(n_components=4)\n",
    "pcan_data4.fit(origin_noisyless)\n",
    "pcant_data4 = pcan_data4.transform(origin_data4)\n",
    "pcan_data4_recons = np.dot(pcant_data4, pcan_data4.components_) + pcan_data4.mean_\n",
    "print(mean_squared_error(pcan_data4_recons, origin_noisyless)*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.542470666666669\n",
      "1.9177677499460617\n",
      "3.331722103940329\n",
      "4.5482571972498285\n",
      "5.139266666666667\n"
     ]
    }
   ],
   "source": [
    "#noiseless for data5\n",
    "pcan_data5 = PCA(n_components=0)\n",
    "pcan_data5.fit(origin_noisyless)\n",
    "pcant_data5 = pcan_data5.transform(origin_data5)\n",
    "pcan_data5_recons = np.dot(pcant_data5, pcan_data5.components_) + pcan_data5.mean_\n",
    "print(mean_squared_error(pcan_data5_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data5 = PCA(n_components=1)\n",
    "pcan_data5.fit(origin_noisyless)\n",
    "pcant_data5 = pcan_data5.transform(origin_data5)\n",
    "pcan_data5_recons = np.dot(pcant_data5, pcan_data5.components_) + pcan_data5.mean_\n",
    "print(mean_squared_error(pcan_data5_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data5 = PCA(n_components=2)\n",
    "pcan_data5.fit(origin_noisyless)\n",
    "pcant_data5 = pcan_data5.transform(origin_data5)\n",
    "pcan_data5_recons = np.dot(pcant_data5, pcan_data5.components_) + pcan_data5.mean_\n",
    "print(mean_squared_error(pcan_data5_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data5 = PCA(n_components=3)\n",
    "pcan_data5.fit(origin_noisyless)\n",
    "pcant_data5 = pcan_data5.transform(origin_data5)\n",
    "pcan_data5_recons = np.dot(pcant_data5, pcan_data5.components_) + pcan_data5.mean_\n",
    "print(mean_squared_error(pcan_data5_recons, origin_noisyless)*4)\n",
    "\n",
    "pcan_data5 = PCA(n_components=4)\n",
    "pcan_data5.fit(origin_noisyless)\n",
    "pcant_data5 = pcan_data5.transform(origin_data5)\n",
    "pcan_data5_recons = np.dot(pcant_data5, pcan_data5.components_) + pcan_data5.mean_\n",
    "print(mean_squared_error(pcan_data5_recons, origin_noisyless)*4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
