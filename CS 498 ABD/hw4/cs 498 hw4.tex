\def\solutionMode{TRUE}%

\documentclass[11pt]{article}%
\usepackage{solutions}%
\usepackage{374}%
\usepackage{374_extra}% 
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\begin{document}

\noindent\textbf{\LARGE HW{4} Solution}\\
\noindent{\textbf{\Course: \CourseName, \Semester}}
\hfill\Version{1.0}%
\\[-0.12cm]
%
\Hr%
\smallskip%

\noindent%
Submitted by:
\begin{compactitem}
    \item \textbf{$\ll$Ray Ying$\gg$}:
    \textbf{$\ll$xinruiy2$\gg$}
    \item \textbf{$\ll$Aditya Pillai$\gg$}:
    \textbf{$\ll$apillai4$\gg$}
\end{compactitem}
\Hr
\medskip
\SaveIndent%

\begin{questions}[1]
\item 
\begin{enumerate}
    \item   
        \begin{enumerate}
            \item For each column of $\Pi$, there is only one entry has value $\pm 1$. The probability for each entry to be non-zero is the same, $P = \frac{1}{m}$.\\ 
            $\mathbb{E}[((\Pi x)[j])^2] = \sum_{i = 0}^{n} \frac{1}{m} \cdot (\pm 1)^2 \cdot x[i]^2 $, $j \in {1, ... ,m}$ (columns are independent)\\
            $\mathbb{E}[\norm{\Pi x}_2^2] = \sum_{j = 0}^{m} \mathbb{E}[((\Pi x)[j])^2] = m \cdot ( \sum_{i = 0}^{n} \frac{1}{m} \cdot (\pm 1)^2 \cdot x[i]^2) = \norm{x}_2^2$ (Linearity of expectation)\\
        
            Let $Z = \norm{\Pi x}_2^2$, we proved that $\mathbb{E}[Z] = \norm{x}_2^2$ \\
            
            $\mathbb{E}[Z^2] = \sum_{j \neq j' \in [1,..,m]} E[Z_{j'}\cdot Z_j] = \sum_{j \in [1,..,m]} E[Z_j^2] + 2 \cdot \sum_{j<j' \in [1,...,m]}E[z_jz_j']$\\
            
            $Var[Z] = \mathbb{E}[Z^2] - \mathbb{E}[Z]^2 = 2 \cdot \sum_{j<j' \in [1,...,m]}E[z_jz_j'] \leq 2 \mathbb{E}[Z]^2$. Because we are doing averaging tricks through the rows as the expectation of each row is the $Z/m$. $Var[Z] \leq \frac{2}{m}\mathbb{E}[Z]^2$, if $m = \frac{6}{\epsilon^2}$, $Var[Z] \leq \frac{\epsilon^2}{3}\mathbb{E}[Z]^2$. \\
            
            By Chebyshev's inequality, $Pr[|Z - \mathbb{E}(Z)| \geq \epsilon \norm{x}_2^2] \leq 1/3 \implies$ with $2/3$ probability that we satisfied the given inequality.\\
            
            \item Because part $1$ we used fully independent function to generate the bits, we only need pairwise independent function for expectation and variance. Therefore, we only need $\log{n} + \log{m}$ bits for the the pairwise independence function. As $m < n$, $\log{n} + \log{m} < 2 \cdot \log{n} \implies$ $O(\log{n})$ bits needed.
        \end{enumerate}
    \item \\
    
    \item \begin{enumerate}
        \item The schema starts with $1$ set of bit vectors with size $s$ which is large, each bit vector has $d$ bits. Essentially, it's a matrix of shape $s \times d$.\\
        
        We randomly permutation of the bits in each row and make $N = O(n^{1/(1+\epsilon)})$ matrices. Then, for each matrix, we sort rows by lexicographic order for performing binary search.\\
        
        We use two pointers that keep track of the longest prefix matches the given input bit vector $q$ through binary search, one pointer moves up and the other moves down. This will gives $2$ bit vector per matrix, which in total gives $2N = O(n^{1/(1+\epsilon)})$ bit vectors.\\
        
        Follow the analysis from lecture, for hamming cube, we know that $\rho = \frac{In 1/p_1}{In 1/p_2}$ where $p_1 = 1 - \frac{r}{d}$, $p_2 = 1 - \frac{r(1+\epsilon)}{d}$ ($r$ be the distance). Define $k = log_{1/p_2}$,  $n^\rho = O(n^{1/(1+\epsilon)})$, the probability that a bit vector will agree in the first $k$ bits with $p$ will be $p_1^k = n^{-\rho}$, that given $n^\rho$ such permutations, with constant probability that exists one bit vector agrees with $p$ on the first $k$ coordinates. \\
        
        Similarly, is a point $q'$ is at $r(1+\epsilon)$ from $q$, the probability of any coordinate agrees with $p$ is at most $p_2$, therefore, the probability of agree on first $k$ coordinates is $p_2^k = \frac{1}{n}$. Same as $N$ matrix, the expected number of rows that agrees on first $k$ coordinates are at most $N$, by Markov, the probability that bad coordinates are less than $2N$ is greater than $\frac{1}{2}$.\\
        
        Hence, in this schema, overall having $N$ permutations will have more than $\frac{1}{2}$ probability that finds the nearest neighbor agrees in $k$ coordinates. In this schema, we don't have to keep different structure for different $r$.
    \end{enumerate}
    \\
    
    \item
\end{enumerate}    
\end{questions}


\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
